---
title: "Chapter 1 - Matrix Algebra"
format: html
---

```{r setup}
#| echo: false
#| message: false
#| warning: false
rm(list=ls())
library(matlib)
```

## Basic Definitions


## Properties of Matrices


## Linear independence


## Rank


## Orthogonal Matrices



## Exercises

For exercises 1-5, let

$$
X = \left[ {\begin{array}{cc}
    2 & 3\\
    -1 & 4\\
    \end{array}} \right ];\quad
Y = \left[ {\begin{array}{ccc}
    2 & 0 & 1\\
    1 & -2 & 3\\
    \end{array}} \right ];\quad
Z = \left[ {\begin{array}{cc}
    1 & 1\\
    -1 & 1\\
    0 & 2 \\
    \end{array}} \right ]
$$

$$
W = \left [{\begin{array}{cc}
    1 & 0 \\
    8 & 3 \\
\end{array}}\right];\quad
T = \left [{\begin{array}{cc}
    1 & 0 \\
    0 & 1 \\
\end{array}} \right]
$$

### Problem 1
> If possible, perform each matrix operation. If the indicated operation is not possible, explain why.

a.) $X + Y$ - This operation is not possible because $X$ and $Y$ do not have the same dimensions.
b.) $X + W$
```{r}
X <- matrix(c(2,3,
              -1,4), ncol = 2, nrow = 2, byrow = TRUE)
W <- matrix(c(1,0,
              8,3), ncol = 2, nrow = 2, byrow = TRUE)

X + W

```
c.) $X - T$
```{r}
T <- matrix(c(1,0,
              0,1), ncol = 2, nrow = 2, byrow = TRUE)

```
$$
X - T = \left[
 \begin{array}{rr}
  1 & 3 \\ 
  -1 & 3\ \\ 
  \end{array}
\right]
$$
d.) $XY$

```{r}
Y <- matrix(c(2,0,1,
              1,-2,3), 
            ncol = 3, nrow = 2, byrow = TRUE)

# printMatrix(X%*%Y, latex = TRUE, tol = 8)
```

$$
\left[
 \begin{array}{rrr}
  7 & -6 & 11 \\ 
  2 & -8 & 11 \\ 
  \end{array}
\right]
$$
e.) $YX$ - This operation is not allowed as $Y$ is a $2\times 3$ matrix and $X$ is $2\times2$. The matrices are not conformable in this order.

f.) $3X$ - is simple scalar multiplication.

$$
3X = \left[ {\begin{array}{cc}
    3*2 & 3*3\\
    3*(-1) & 3*4\\
    \end{array}} \right ] = 
    \left[ {\begin{array}{cc}
    6 & 9\\
    -3 & 12\\
    \end{array}} \right ]
$$

g.) $XT$ - $T$ is just the $2\times2$ identity matrix so $XT = X$.

h.) $TX$ - Again, $T$ is just the identity matrix and can be applied to either "side" of $X$ in a matrix multiplication operation to yield $TX = X$.

i.) $X + (Y+Z)$ - None of these matrices have the same dimensions so this operation is not defined.

j.) $Z(T + W)$ - This operation is defined.

```{r}
#|echo: false
Z <- matrix(c(1,1,
              -1,1,
              0,2), ncol = 2, nrow = 3, byrow = TRUE)

# printMatrix(Z %*% (T + W), latex = TRUE, tol = 7)
```
$$
\left[
 \begin{array}{rr}
  10 & 4 \\ 
  6 & 4 \\ 
  16 & 8 \\ 
  \end{array}
\right]
$$
k.) $Y(T + W)$ - This operation is not defined because $Y$ does not conform with $T+W$.

### Problem 2
> Find $(W + T)Y$, $WY$, and $TY$. Show that $(W+T)Y = WY + TY$, thus illustrating property 5 for matrix operations.

```{r}
(W + T) %*% Y
```

```{r}
W %*% Y
```

```{r}
T%*%Y
```
From these results, it is clear that $(W+T)Y = WY + TY$.

### Problem 3
> Find the zero matrix associated with $X$; of $Y$. Can you generalize to describe the zero matrix for any $n\times k$ matrix?

The zero matrix is defined such that $X + 0 = 0 + X = X$. The matrix that satisfies this condition for $X$ is the $2\times2$ $0$ matrix:
$$
\left[{
\begin{array}{cc}
0 & 0 \\
0 & 0 \\
\end{array}
} \right]
$$
For $Y$, the appropriate zero matrix is
$$
\left[{
\begin{array}{ccc}
0 & 0 & 0\\
0 & 0 & 0\\
\end{array}
} \right]
$$
In general, the zero matrix for any $n\times k$ matrix will be the matrix of the same dimensions with zeroes for all entries.

### Problem 4
> Find the negative of $X$; of $Y$. Can you generalise to describe the form of the negative matrix for any $n\times k$ matrix?

The negative matrix, $N$ of a matrix $X$ satisfies the condition that $X + N = 0$. We can obtain this matrix by scalar multiplication by $-1$.

$X + (-1)X = 0$

$$
(-1)X = \left[
 \begin{array}{rr}
  -2 & -3 \\ 
  1 & -4 \\ 
  \end{array}
\right]
$$

```{r}
#|echo: false
# printMatrix(-1*Y, latex = TRUE, tol=7)
```
$$
(-1)Y = 
\left[
 \begin{array}{rrr}
  -2 & 0 & -1 \\ 
  -1 & 2 & -3 \\ 
  \end{array}
\right]
$$
In general, the negative of a matrix, $A$, is $(-1)A$.

### Problem 5
> Find $2Y$, $X(2Y)$, $XY$, and then $2(XY)$. Show that $X(2Y)=2(XY)$, thus illustrating property 6 for matrix operations.

```{r}
#|echo: false
# printMatrix(X %*% (2*Y), latex = TRUE, tol=7)
```

$$
2Y = \left[
 \begin{array}{rrr}
  4 & 0 & 2 \\ 
  2 & -4 & 6 \\ 
  \end{array}
\right]
$$

$$
X(2Y) = \left[
 \begin{array}{rrr}
  14 & -12 & 22 \\ 
  4 & -16 & 22 \\ 
  \end{array}
\right]
$$

$$
XY = \left[
 \begin{array}{rrr}
  7 & -6 & 11 \\ 
  2 & -8 & 11 \\ 
  \end{array}
\right]
$$

$$
2(XY) = \left[
 \begin{array}{rrr}
  14 & -12 & 22 \\ 
  4 & -16 & 22 \\ 
  \end{array}
\right]
$$
These operations show that we can pull the scalar out in front and perform the matrix operation first, or perform the scalar multiplication first and then do the matrix multiplication.

### Problem 6
> Let
$$
X = \left [ {
\begin{array}{ccc}
x_{11} & x_{12} \\
x_{21} & x_{22} \\
x_{31} & x_{32} \\
\end{array}
} \right ]\quad\text{and}\quad
Y = \left[{\begin{array}{cccc}
y_{11} & y_{12} & y_{13} & y_{14} \\
y_{21} & y_{22} & y_{23} & y_{24} \\
\end{array}}\right]
$$
> What are the dimensions of $XY$? Using Definition 1.3.3, find the entry in row 1 and column 1 of the product $XY$. Also find the entry in row 2 and column 3 of the product.

The dimensions of $XY$ will be $3\times4$. Generically, if $X$ is $m\times n$ and $Y$ is $n\times k$, then product of $X$ and $Y$ will be a matrix that is $m\times k$.

If $P = XY$, then $p_{11} = x_{11}y_{11} + x_{12}y_{21}$. $p_{23} = x_{21}y_{13} + x_{22}y_{23}$.

Here's a matrix to verify the results above.
```{r}
X <- matrix(c(2,1,
              3,2,
              1,4), ncol = 2, nrow = 3, byrow=TRUE)

Y <- matrix(c(3,4,0,1,
              1,3,1,1), ncol = 4, nrow = 2, byrow = TRUE)

X %*% Y

```
### Problem 7
> Let $\vec{x}^T = [x_1\quad x_2\quad x_3]$ and $\vec{y}^T = [y_1\quad y_2\quad y_3]$. What are the dimensions of $\vec{x}^T\vec{y}$? of $\vec{y}\vec{x}^T$? Find the expression for $\vec{x}^T\vec{y}$ and evaluate this expression when $x_1 = 3, x_2=-1$, and $x_3 = 6$.

Since $\vec{x}^T$ is a $1\times 3$ matrix and $\vec{y}$ is a $3\times 1$ matrix, the result of their product will be a $1\times 1$ matrix or a scalar. The dimensions of $\vec{y}\vec{x}^T$ will be a $3\times 3$ matrix by the same logic.
$$
\vec{x}^T\vec{y} = [x_1\quad x_2\quad x_3]\left [{
\begin{array}{c}
y_1 \\
y_2 \\
y_3 \\
\end{array}
} \right ] = x_1y_1 + x_2y_2 + x_3y_3
$$
When $\vec{x}^T = [3\quad -1\quad 6]$, the $\vec{x}^T\vec{y} = 3y_1 - y_2 + 6y_3$.

### Problem 8
> If define $X^2$ to mean $X$ multiplied by itself, what must be true of $X$ in order for this matrix to exist?

In order for this to work, $X$ must be a square matrix. If $X$ is an $m\times n$ matrix where $m\neq n$, then the columns of the first copy won't match the rows of the second copy and matrix multiplication will be undefined for $X$.

### Problem 9


### Problem 10
> Let
> $$
X = \left [ {
\begin{array}{ccc}
2 & 1 & 1 \\
1 & -1 & 3 \\
0 & 1 & 2 \\
\end{array}
} \right ]\quad\text{and}\quad
Y = \left[{\begin{array}{ccc}
1 & 1 & 0 \\
0 & 2 & 1 \\
-1 & 2 & 3
\end{array}}\right]
$$
Find $X^T, Y^T, X^T + Y^T, X + Y, \text{ and } (X + Y)^T$. Verify that $(X + Y)^T = X^T + Y^T$.

```{r}
X <- matrix(c(2,1,1,
              1,-1,3,
              0,1,2), ncol = 3, nrow = 3, byrow = TRUE)

Y <- matrix(c(1,1,0,
              0,2,1,
              -1,2,3), ncol = 3, nrow = 3, byrow = TRUE)
```

Find $X^T$ and $Y^T$.
```{r}
X_t <- t(X)
Y_t <- t(Y)
```

Find $X^T + Y^T$
```{r}
X_t + Y_t
```
Find $X + Y$
```{r}
X + Y
```
Find $(X + Y)^T$
```{r}
t(X + Y)
```

The results above confirm that $X^T + Y^T = (X + Y)^T$.

### Problem 11
> Let
> $$
X = \left [ {
\begin{array}{cc}
1 & 1 \\
1 & 1 \\
\end{array}
} \right ]\quad\text{and}\quad
Y = \left[{\begin{array}{ccc}
1 & 3 & 0 \\
2 & 5 & 1 \\
\end{array}}\right]
$$
Find $X^T, Y^T, Y^TX^T, XY, \text{ and } (XY)^T$. Verify that $Y^TX^T = (XY)^T$.

```{r}
X <- matrix(c(1,1,
              1,1), ncol = 2, nrow = 2, byrow = TRUE)

Y <- matrix(c(1,3,0,
              2,5,1), ncol = 3, nrow = 2, byrow = TRUE)
```

Since $X$ is a symmetric matrix, $X^T = X$.
```{r}
X_t <- t(X)
X_t
```


Find $Y^T$
```{r}
Y_t <- t(Y)
Y_t
```
Find $Y^TX^T$
```{r}
Y_t %*% X_t
```
Find $XY$
```{r}
X %*% Y
```
Find $(XY)^T$
```{r}
t(X %*% Y)
```

These results confirm that $Y^TX^T = (XY)^T$.

### Problem 12
> Let $X$, $Y$, and $Z$ be conformable. Prove that $(XYZ)^T = Z^TY^TX^T$.

We can show this by leveraging the previous result. Let $W = XY$. We can then rewrite the equation as $(WZ)^T = Z^TW^T$. This follows from the property of transposes. But since $W=XY$, it follows that $W^T = Y^TX^T$. Thus, by substitution, we get that $(XYZ)^T = Z^TY^TX^T$.

### Problem 13
> Let
> $$
X = \left [ {
\begin{array}{cccc}
3 & 0 & 8 & -2 \\
1 & 2 & 5 & 0 \\
\end{array}
} \right ]
$$
What are the dimensions of $X^T$? Find $X^T$. Is $X$ symmetric?

Since $X$ is a $2\times 4$ matrix, $X^T$ will be $4\times 2$.

$$
X^T = \left [ {
\begin{array}{cc}
3 & 1 \\
0 & 2 \\
8 & 5 \\
-2 & 0 \\
\end{array}
} \right ]
$$
$X$ cannot be symmetric because it is not a square matrix. $X\neq X^T$.

### Problem 14
> Let
> $$
X = \left [ {
\begin{array}{ccc}
2 & 1 & 4 \\
1 & -3 & x_{23} \\
4 & 2 & 0
\end{array}
} \right ]
$$
What must $x_{23}$ equal in order for $X$ to be symmetric?

$x_{23}$ must be $2$ for $X$ to be a symmetric matrix.

### Problem 15
> Let
> $$
X = \left [ {
\begin{array}{c}
0 \\
1 \\
-1 
\end{array}
} \right ]
$$
Find $\vec{x}^T\vec{x}$ and verify the statement of Theorem 1.2.2.

$\vec{x}^T\vec{x} = x_1^2 + x_2^2 + x_3^2 = 0 + 1 + 1 = 2$. We expect a scalar because we have a $1\times 3$ matrix multiplied by a $3\times 1$ matrix resulting in a $1\times 1$ matrix.

### Problem 16
> Let
> $$
x = \left [ {
\begin{array}{c}
2 \\
4 \\
1 
\end{array}
} \right ]
$$
Find $\vec{x}\vec{x}^t$ and verify the statement of Theorem 1.2.3.

```{r}
x <- c(2,4,1)
x_t <- t(x)

x %*% x_t
```
Theorem 1.2.3 states that $\vec{x}\vec{x}^t$ results in a symmetric matrix with squares on the diagonal and cross products elsewhere. The resulting matrix is indeed a symmetric $3\times 3$ matrix with $x_1^2, x_2^2,x_3^2$ on the diagonal and cross products elsewhere.

### Problem 17
> Prove Theorem 1.2.2

Theorem 1.2.2 states that if
$$
x = \left [ {
\begin{array}{c}
x_1 \\
x_2 \\
\vdots \\
x_n 
\end{array}
} \right ]
$$
then,
$$
\vec{x}^T\vec{x} = \sum^n_{i=1} x_i^2
$$
This result follows directly from the way that matrix multiplication is defined.

$$
\vec{x}^T\vec{x} = [x_1\quad x_2\quad \cdots\quad x_n]\left [{
\begin{array}{c}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{array}
} \right ] = x_1^2 + x_2^2 + \cdots x_n^2 = \sum_{i=1}^n x_i^2
$$

This operation pairs each entry with itself in a product and the squares are summed to produce the one and only entry in the new matrix.

### Problem 18
> Prove Theorem 1.2.3

Theorem 1.2.3 states that if
$$
x = \left [ {
\begin{array}{c}
x_1 \\
x_2 \\
\vdots \\
x_n 
\end{array}
} \right ]
$$
then $\vec{x}\vec{x}^T results in an $n\times n$ symmetric matrix with squares on the diagonal and cross products elsewhere.

When we carry out the matrix multiplication we expect to get a $3\times 3$ matrix.

$$
\vec{x}\vec{x}^T = \left [{
\begin{array}{c}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{array}
} \right ][x_1\quad x_2\quad \cdots\quad x_n] = \left[{\begin{array}{ccc} 
x_1^2 & x_2x_1 & \cdots & x_nx_1\\
x_1x_2 & x_2^2 & \cdots & x_nx_2\\
\vdots & \vdots & \ddots & \vdots \\
x_1x_n & x_2x_n & \cdots & x_n^2\\
\end{array}} \right]
$$
Since scalar multiplication is commutative, $x_ix_j = x_jx_i$ so we can readily see that the cross products get reflected across the diagonal which is indeed composed of the square of the entries of the original vector, $\vec{x}$. 

### Problem 19
> Let
> $$
x = \left [ {
\begin{array}{ccc}
2 & 0 & 1 \\
1 & 3 & -1 \\
\end{array}
} \right ]
$$
What are the dimensions of $X^TX$? Find $X^TX$ by inspection. Verify your answer by direct multiplication. Is $X^TX$ symmetric as expected?

$X^TX$ will be a $3\times 3$ matrix. Since this is a square matrix, it has the potential to be symmetric.

By inspection, it's not obvious that it will be symmetric.
$$
\left [ {
\begin{array}{cc}
2 & 1 \\
0 & 3 \\
1 & -1 \\
\end{array}
} \right ]
\left [ {
\begin{array}{ccc}
2 & 0 & 1 \\
1 & 3 & -1 \\
\end{array}
} \right ]
$$

```{r problem 19}
X <- matrix(c(2,0,1,
              1,3,-1), ncol = 3, nrow = 2, byrow = TRUE)
X_t <- t(X)

X_t %*% X
```
Direct inspection of the matrix multiplication shows that the matrix product is, in fact, symmetric.

### Problem 20
Not terribly interesting and similar to problem 19. Skipping.

### Problem 21
Not terribly interesting and similar to problem 19. Skipping.

### Problem 22
> Prove Theorem 1.2.1.

This result flows directly from the way that matrix multiplication is defined. Since the columns of $X$ are the rows of $X^T$, we get something that looks like this:
$$
X = \left[{\begin{array}{cccc} 
x_{11} & x_{12} & \cdots & x_{1k}\\
x_{21} & x_{22} & \cdots & x_{2k}\\
\vdots & \vdots & \ddots & \vdots \\
x_{n1} & x_{n2} & \cdots & x_{nk}\\
\end{array}} \right];\quad X^T = \left[{\begin{array}{cccc} 
x_{11} & x_{21} & \cdots & x_{n1}\\
x_{12} & x_{22} & \cdots & x_{n2}\\
\vdots & \vdots & \ddots & \vdots \\
x_{k1} & x_{k2} & \cdots & x_{kn}\\
\end{array}} \right]
$$
Since the rows of $X^T$ are the columns of $X$ when we take the dot product of row $i$ of $X^T$ with column $i$ of $X$, each term will be paired with it's "twin", resulting in the squared terms on the diagonal of $X^TX$. When we take the dot product of row $i$ of $X^T$ with column $j$ of $X$, $i\neq j$, then we get a sum of cross products as indicated by the theorem. 

$$
X^TX = \left[{\begin{array}{ccc} 
\sum_{i=1}^n x_{i1}^2 & \sum_{i=1}^n x_{i1}x_{i2} & \cdots & \sum_{i=1}^n x_{i1}x_{ik} \\
\sum_{i=1}^n x_{i2}x_{i1} & \sum_{i=1}^n x_{i2}^2 & \cdots & \sum_{i=1}^n x_{i2}x_{ik}\\
\vdots & \vdots & \ddots & \vdots \\
\sum_{i=1}^n x_{ik}x_{i1} & \sum_{i=1}^n x_{ik}x_{i2} & \cdots & \sum_{i=1}^n x_{ik}^2\\
\end{array}} \right]
$$

### Problem 23
> Suppose that $X$ is an $n\times k$ matrix whose first column is a column of ones. What will be the first main diagonal entry of $X^TX$?

We can think of this operation as the dot product of the first row vector of $X^T$ and the first column vector of $X$. This results in $1^1 + 1^2 + \cdots + 1^2 = \sum_{i=1}^n 1^2 = n$. Thus, the first entry on the main diagonal will be $n$. 

### Problem 24
We can leverage the proof of Theorem 1.2.1. Notice that The columns of $X^TX$ are equal to the rows. This means that if we were to transpose $X^TX$, the resulting matrix would be unchanged. This shows that $(X^TX)^T = X^TX$. We can also show this by distributing the transpose operator: $(X^TX)^T = X^T(X^T)^T = X^TX$. Note that when the transpose operator is distributed, the matrices inside that parentheses must have their order reversed so that they conform.

### Problem 25
> Show that if $X$ and $Y$ are both $n\times n$ symmetric matrices that $X+Y$ and $X-Y$ are also symmetric.

Since for $X$, $x_{ij} = x_{ji}$ and the same is true for $Y$ when we add or subtract these two matrices, the resulting entries $(x-y)_{ij} = (x-y)_{ji}$ and $(x+y)_{ij} = (x+y)_{ji}$. Simple $2\times 2$ matrices illustrate the point.

$$
X = \left[
 \begin{array}{rr}
 x_{11} & x_{12} \\ 
  x_{21} & x_{22} \\ 
  \end{array}
\right];\quad Y = \left[
 \begin{array}{rr}
 y_{11} & y_{12} \\ 
  y_{21} & y_{22} \\ 
  \end{array}
\right]
$$
$$
X + Y = \left[
 \begin{array}{rr}
 x_{11}+y_{11} & x_{12}+y_{12} \\ 
 x_{21}+y_{21} & x_{22}+y_{22} \\ 
  \end{array}
\right]
$$
Since $x_{12} = x_{21}$ and $y_{12} = y_{21}$, the sums $x_{12}+y_{12} = x_{21}+y_{21}$ showing that $X+Y$ is also symmetric. The same reasoning can be applied to $X-Y$ to show that it must also be a symmetric matrix.

### Problem 26
> Let
$$
X = \left[
 \begin{array}{rr}
 2 & 4 & 8 \\ 
 1 & 3 & 2 \\ 
  \end{array}
\right]
$$
Show that
$$
I = \left[
 \begin{array}{rrr}
 1 & 0 & 0 \\ 
 0 & 1 & 0 \\
 0 & 0 & 1 \\
  \end{array}
\right]
$$
is the right identity for $X$. What are the dimensions of its left identity? Does it have an overall identity?

Because it is not a square matrix, $X$ does not have an overall identity. The left identify of $X$ will need to be a $2\times 2$ matrix because $X$ is $2\times 3$ and the multiplication must also result in a $2\times 3$ matrix, namely $X$. Thus, the left identity should be
$$
I = \left[
 \begin{array}{rr}
 1 & 0 \\ 
 0 & 1 \\ 
  \end{array}
\right]
$$

We can confirm this with a quick test.
```{r}
X <- matrix(c(2,4,8,
              1,3,2), ncol=3, nrow=2, byrow=TRUE)
I <- matrix(c(1,0,
              0,1), ncol=2,nrow=2, byrow=TRUE)

I %*% X
```
This shows that the reasoning is correct. The pattern to notice here is that the left identity will be a square matrix with the number of rows and columns equal to the number of rows of $X$ and the right identity will be a square matrix with the number of rows and columns equal to the number of columns of $X$.

### Problem 27
> Find the inverse of
$$
X = \left[
 \begin{array}{rrr}
 3 & 0 & 0 \\ 
 0 & 7 & 0 \\
 0 & 0 & 2 \\
  \end{array}
\right]
$$
and verify your answer.

The inverse of $X$ is the matrix that when multiplied with $X$ produces the $3\times 3$ identity matrix. This matrix should be the reciprocals for the diagonal entries.

$$
X^{-1} = \left[
 \begin{array}{rrr}
 1/3 & 0 & 0 \\ 
 0 & 1/7 & 0 \\
 0 & 0 & 1/2 \\
  \end{array}
\right]
$$

We can verify this with R code.
```{r}
X <- matrix(c(3,0,0,
              0,7,0,
              0,0,2), ncol=3,nrow=3,byrow=TRUE)
inv(X)
```
The result above confirms this intuition.

### Problem 28
> Let
$$
X = \left[
\begin{array}{rr}
 0 & 0 \\ 
 6 & 1 \\
\end{array}
\right]
$$
a.) Show that
$$
X^{-1} = \left[
\begin{array}{rrr}
 1/3 & 0 \\ 
 -2 & 0 \\
\end{array}
\right]
$$

To find $X^{-1}$, we need to solve the following equation:
$$
\left[
\begin{array}{rr}
 3 & 0 \\ 
 6 & 1 \\
\end{array}
\right]
\left[
\begin{array}{rr}
 a & b \\ 
 c & d \\
\end{array}
\right] = 
\left[
\begin{array}{rr}
 1 & 0 \\ 
 0 & 1 \\
\end{array}
\right]
$$

This matrix multiplication results in the following linear equations:
$$
3a = 1,\quad 3b = 0,\quad 6a + c = 0,\quad 6b + d = 1
$$
The solutions are $a=1/3,\quad b=0,\quad c=-2,\quad d=1$. When these values are plugged into the inverse matrix, we get the values indicated above.

> b.) Let
$$
A=\left[
\begin{array}{rr}
 a & b \\ 
 c & d \\
\end{array}
\right]
$$
be any invertible $2\times 2$ matrix. By direct manipulation, show that
$$
A^{-1}=\left[
\begin{array}{rr}
 \frac{d}{ad-bc} & \frac{-b}{ad-bc} \\ 
 \frac{-c}{ad-bc} & \frac{a}{ad-bc} \\
\end{array}
\right]
$$

To show this, we again solve a matrix multiplication problem.

$$
\left[
\begin{array}{rr}
 a & b \\ 
 c & d \\
\end{array}
\right]
\left[
\begin{array}{rr}
 x & y \\ 
 z & w \\
\end{array}
\right] = 
\left[
\begin{array}{rr}
 1 & 0 \\ 
 0 & 1 \\
\end{array}
\right]
$$

Four equations result from this and so we want to solve for $x,y,z,\text{ and } w$ in terms of $a,b,c,\text{ and }d$. The equations are
$$
\begin{align}
ax + bz = 1\\
cx + dz = 0\\
ay + bw = 0\\
cy + dw = 1
\end{align}
$$
We start with the first two, solving for $x$ and $z$.

$$
\begin{align}
ax + bz = 1\\
cx + dz = 0\\
x = \frac{-dz}{c}\\
\frac{-adz}{c} + bz = 1\\
z(b - \frac{ad}{c}) = 1\\
z(bc - ad) = c\\
z = \frac{c}{bc - ad} = \frac{-c}{ad- bc}\\
x = -\frac{d}{c}\frac{(-c)}{ad-bc} = \frac{d}{ad-bc}
\end{align}
$$
We can then apply the same proceedure to solve for $y$ and $w$.
$$
\begin{align}
ay + bw = 0\\
cy + dw = 1\\
y = \frac{-bw}{a}\\
\frac{-cbw}{a} + dw = 1\\
w(d - \frac{cb}{a}) = 1\\
w(ad - bc) = a\\
w = \frac{a}{ad-bc}\\
y = -\frac{-b}{a}\frac{(a)}{ad-bc} = \frac{-b}{ad-bc}
\end{align}
$$
Now we can plug these values in for the corresponding entries in the matrix:
$$
A^{-1}=\left[
\begin{array}{rr}
  \frac{d}{ad-bc} & \frac{-b}{ad-bc} \\ 
 \frac{-c}{ad- bc} &  \frac{a}{ad-bc} \\
\end{array}
\right]
$$
> c.) Let
$$
A=\left[
\begin{array}{rr}
 2 & 4 \\ 
 1 & -3 \\
\end{array}
\right]
$$
Use the algorithm of part b to find $A^{-1}$.

First, we need to compute $ad-bc = 2*(-3) - 4(1) = -10$.

plugging this into the matrix, we get
$$
A^{-1}=\left[
\begin{array}{rr}
 \frac{-3}{-10} & \frac{-4}{-10} \\ 
 \frac{-1}{-10} & \frac{2}{-10} \\
\end{array}
\right]=
\left[
\begin{array}{rr}
 0.3 & 0.4 \\ 
 0.1 & -0.2 \\
\end{array}
\right]
$$

We can confirm this with R code.
```{r}
A <- matrix(c(2,4,
              1,-3), ncol=2,nrow=2,byrow=TRUE)

inv(A)
```
This confirms that the algorithm was properly applied.

### Problem 29
> Prove that $(XY)^{-1} = Y^{-1}X^{-1}$.

Start by multiplying both sides of the equation by $(XY)$.
$$
(XY)(XY)^{-1} = (XY)Y^{-1}X^{-1}
$$
This simplifies to
$$
\begin{align}
I = X(YY^{-1})X^{-1}\\
I = XIX^{-1}\\
I = XX^{-1}\\
I = I
\end{align}
$$

This demonstrates that the two sides of the original equation are identical.

### Problem 30
>Prove that if $X$, $Y$, and $Z$ are conformable and nonsingular then $(XYZ)^{-1} = Z^{-1}Y^{-1}X^{-1}$.

This is just an extension of the previous problem. If $X$ and $Y$ are nonsingular and conformable, the their product is nonsingular and conformable with $Z$. Let $W = XY$, then we have already shown that $WZ = Z^{-1}W^{-1}$ and we know what $W^{-1} = Y^{-1}X^{-1}$ and this demonstrates that $(XYZ)^{-1} = Z^{-1}Y^{-1}X^{-1}$.

Alternatively, just left multiply both sides by $XYZ$ and you can show that this assertion is true.

### Problem 31
> Prove that $(X^T)^{-1} = (X^{-1})^T$.

To do this, we will need to leverage the distributive property of the transpose, but going backwards. We start by simplifying the left side of the equation to the identity matrix and attempt to demonstrate equality after applying the simplifying steps.

$$
\begin{align}
(X^T)(X^T)^{-1} = X^T(X^{-1})^T \\
I = (X^{-1}X)^T\quad\text{this step is the crux.} \\
I = I^T = I
\end{align}
$$
By applying a left matrix multiplication by $X^T$ to both sides of the initial equation, we can show that they are, in fact, equivalent.

### Problem 32
> Use the computational shortcut from section 1.2 to show that
$$
X = \left[{
\begin{array}{cc}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
\frac{-1}{\sqrt{2}} & \frac{1}{\sqrt{2}}
\end{array}
}\right]
$$

We can use the dot product, $\vec{x_1}^T\cdot\vec{x_2}$, to investigate this. If the dot product is zero, the two column vectors are orthogonal.

$$
\bigg[\frac{1}{\sqrt{2}}\quad \frac{-1}{\sqrt{2}}\bigg]\cdot \left[{
\begin{array}{c}
\frac{1}{\sqrt{2}}\\
\frac{1}{\sqrt{2}}
\end{array}
}\right] = \frac{1}{2} - \frac{1}{2} = 0
$$
So the two vectors are, in fact, orthogonal and form an orthogonal basis for $\mathbb{R}^2$.

### Problem 33
> Is
$$
X = \left[{\begin{array}{cc}
-1 & 1\\
1 & 1
\end{array}
}\right]
$$
orthogonal? If not, what value of $c$ makes the matrix $cX$ ortgononal?

Just looking at the two column vectors, they appear to be orthogonal.

```{r}
dot_product <- function (x,y) {
  len <- length(x)
  result <- 0
  for (i in 1:len) {
    result <- result + x[i]*y[i]
  }
  result
}

x1 <- c(-1,1) 
x2 <- c(1,1)

dot_product(x1,x2)
```
The dot product is zero, so the two column vectors are orthogonal.

I don't think that there is any value of $c$ that can make the column vectors of a matrix orthogonal to each other. $c$ will just scale the column vectors but will not change the angles between them. 
### Problem 34
>Is
$$
X = \left[{\begin{array}{cccc}
-1 & 1 & -1 & -1 \\
1 & -1 & 1 & -1\\
1 & -1 & -1 & 1\\
1 & 1 & 1 & 1\\
\end{array}
}\right]
$$
orthogonal? If not, what value of $c$ makes the matrix $cX$ orthgononal?

The columns look orthogonal because the negative signs don't match up so the vectors look linearly independent and because of the combos of 1 and -1, they look like they are probably orthogonal. To know for sure, we need to take dot products.

```{r}
x1 <- c(1,1,1,1)
x2 <- c(1,-1,-1,1)
x3 <- c(-1,1,-1,1)
x4 <- c(-1,-1,1,1)

c(
  dot_product(x1,x2),
  dot_product(x1,x3),
  dot_product(x1,x4),
  dot_product(x2,x3),
  dot_product(x2,x4),
  dot_product(x3,x4)
)

```
All of the dot products are zero, so the column vectors are orthogonal.

### Problem 35
> Prove that if $X$ is orthogonal, $X^{-1}$ and $X^T$ are orthogonal.

An orthogonal matrix has the property that $X^T = X^{-1}$ by definition 1.3.3. This means that $X^T = X^{-1}$. Thus, we only need to show that $X^T$ is orthogonal to prove that both are.

If $X^TX = I$, then $XX^T=I$. Recall that the columns of $X^T$ are the rows of $X$. The fact that $XX^T=I$ tells us that the rows of $X$ are orthonormal because we must get a value of 1 when we take the dot product of a row with itself (the corresponding column in the transpose matrix), but we get zeros for all other dot products. We have to in order to get the identity matrix as a result of the matrix multiplication. But if the rows of $X$ are orthonormal, then the columns of $X^T$ are orthonormal. This implies that $X^T$ is an orthogonal matrix.

### Problem 36
> Let
$$
\vec{x} = \left [{
\begin{array}{c}
2 \\
1 \\
3
\end{array}
} \right ]\quad\text{and}\quad \vec{y} = \left [{
\begin{array}{c}
1 \\
2 \\
0
\end{array}
} \right ]
$$
a.) Are $\vec{x}$ and $\vec{y}$ orthogonal?
b.) What is the length of $\vec{x}$? Of $\vec{y}$?

a.) We can compute the dot product of $\vec{x}$ and $\vec{y}$ to determine if they are orthogonal.
$$
\vec{x}^T\cdot\vec{y} = 2*1 + 1*2 + 3*0 = 4
$$
Since $\vec{x}^T\cdot\vec{y} \neq 0$, the two vectors are not orthogonal.

b.) We compute $\vec{x}^T\cdot\vec{x} = 2^2 + 1^2 + 3^2 = 14$. The length of $\vec{x}$ is given by $||\vec{x}|| = \sqrt{14}$. We do the same for $\vec{y}$. $\vec{y}^T\cdot\vec{y} = 1^2 + 2^2 + 0^2 = 5$, so $||\vec{y}|| = \sqrt{5}$.

### Problem 37
> Let
$$
\vec{x_1} = \left [{
\begin{array}{c}
1 \\
1 \\
-1 \\
-1 \\
\end{array}
} \right ], \quad \vec{x_2} = \left [{
\begin{array}{c}
1 \\
-1 \\
1 \\
-1
\end{array}
} \right ]\text{,}\quad \vec{x_3} = \left [{
\begin{array}{c}
1 \\
1 \\
1 \\
1
\end{array}
} \right ]
$$
Is $\{\vec{x_1}, \vec{x_2}, \vec{x_3} \}$ an orthonormal set? If not, find a constant $c$ such that $\{c\vec{x_1}, c\vec{x_2}, c\vec{x_3} \}$ is an orthonormal set. Is the choice of $c$ unique.

```{r}
x1 <- c(1,1,-1,-1)
x2 <- c(1,-1,1,-1)
x3 <- c(1,1,1,1)

c(
  dot_product(x1,x2),
  dot_product(x1,x3),
  dot_product(x2,x3)
)
```
These vectors are orthogonal, but each has a length of 2. 

```{r}
c(
  dot_product(x1, x1),
  dot_product(x2, x2),
  dot_product(x3, x3)
)
```
Therefore, to form an orthonormal set, we need to normalise each vector. We can accomplish this by multiplying each vector by $1/2$.

```{r}
x1 <- 1/2*x1
x2 <- 1/2*x2
x3 <- 1/2*x3

c(
  dot_product(x1, x1),
  dot_product(x2, x2),
  dot_product(x3, x3)
)


```
Now the vectors are orthonormal.

### Problem 38
> Prove Theorem 1.3.1.

At the root, we need to leverage the definition of an orthogonal matrix, that $X^TX = I$. If this is the case, then the columns of $X$ must be an orthonormal set because this is the only way for the matrix product $X^TX$ to be equal to the identity matrix.

### Problem 39
> Prove that if $X$ is orthogonal, then its rows also form an orthonormal set.

We can use the fact that if $X$ is orthongonal, then $XX^T = I$. This matrix multiplication results in taking the dot product of the rows of $X$ and the columns of $X^T$. But the columns of $X^T$ are the rows of $X$. The resulting matrix is the identity matrix. The dot product of the row with its own transpose will be one. All other dot products must be zero in order to produce the identity matrix. This implies that the rows of $X$ are an orthonormal set.

### Problem 40
> Find the eigenvectors associated with eigenvalue $\lambda_2$ of example 1.4.1.

$$
A = \left[{
\begin{array}{cc}
1 & 1\\
-2 & 4
\end{array}
}\right], \lambda_2 = 3
$$
To find the eigenvectors associated with $\lambda_2$, we need to solve the homogeneous matrix equation $[A - 3I]\vec{x} = \vec{0}$.

$$
[A - 3I] = \left[{
\begin{array}{cc}
1 & 1\\
-2 & 4
\end{array}
}\right] - \left[{
\begin{array}{cc}
3 & 0 \\
0 & 3
\end{array}
}\right] = \left[{
\begin{array}{cc}
-2 & 1\\
-2 & 1
\end{array}
}\right]
$$
We can solve the system
$$
\left[{
\begin{array}{cc}
-2 & 1\\
-2 & 1
\end{array}
}\right]\vec{x} = \vec{0}
$$

By row reducing the augmented matrix
$$
\left[{
\begin{array}{ccc}
-2 & 1 & 0\\
-2 & 1 & 0
\end{array}
}\right]
$$

$x_2$ is free and $x_1 = 1/2x_2$. We can write the solution as
$$
\vec{x} = x_2\left[{
\begin{array}{c}
1\\
2
\end{array}
}\right]
$$
or simply,
$$
\vec{x} = \left[{
\begin{array}{c}
1\\
2
\end{array}
}\right]
$$

We can check our result with the following R code:
```{r}
A <- matrix(c(1,1,-2,4), ncol=2, nrow=2, byrow=TRUE)

X <- eigen(A)$vectors
X
```
We want the first vector, associated with $\lambda = 3$. This vector has been normalised, but if we multiply by $-\sqrt{5}$ we can convert it to the vector obtained above.

```{r}
-1*sqrt(5)*X[,1]
```

### Problem 41
> Consider the matrix
$$
A = \left[{
\begin{array}{cc}
1 & -1\\
2 & 4
\end{array}
}\right]
$$
Is there any guarantee that the eigenvalues of this matrix are real? If they are real, find the eigenvalues and eigenvectors associated with $A$.

Since $A$ is not symmetric, there is no guarantee that all eigenvalues are real. We will have to solve the characteristic equation to find the eigenvalues and then solve $A\vec{x} = \lambda\vec{x}$ to find the associated eigenvectors.

The characteristic equation for this matrix is $\lambda^2 - 5\lambda + 6 = 0$, which results in eigenvalues of $\lambda_1 = 3$ and $\lambda_2=2$.

We then solve the homogeneous equation $[A - \lambda_i]\vec{x} = \vec{0}$ for each eigenvalue to obtain the eigenvectors. The eigenvectors are 
$$
\lambda_1: \left[{
\begin{array}{c}
-1\\
2
\end{array}
}\right], \lambda_2: \left[{
\begin{array}{c}
-1\\
1
\end{array}
}\right]
$$

### Problem 42
> Consider the matrix
$$
A = \left[{
\begin{array}{cc}
1 & 1\\
-1 & 2
\end{array}
}\right]
$$
Is there any guarantee that the eigenvalues of this matrix are real? If they are real, find the eigenvalues and eigenvectors associated with $A$.

This matrix is not symmetric so there is no guarantee that all of its eigenvalues are real. We find the eigenvalues with the characteristic equation generate from
$$
\begin{align}
\text{det}\left|{
\begin{array}{cc}
1-\lambda & 1\\
-1 & 2-\lambda
\end{array}}\right| = 0\\
(1-\lambda)(2 - \lambda) - (1)(-1) = 0\\
2 - \lambda - 2\lambda + \lambda^2 + 1 = 0\\
\lambda^2 - 3\lambda + 3 = 0
\end{align}
$$
This characteristic equation does not have any real roots. The two roots are
$$
\frac{3 \pm \sqrt{3}i}{2}
$$

### Problem 43
> Consider the matrix
$$
A = \left[{
\begin{array}{cc}
1 & 1\\
1 & 1
\end{array}
}\right]
$$
Is there any guarantee that the eigenvalues of this matrix are real? If they are real, find the eigenvalues and eigenvectors associated with $A$.

Yes, this matrix is symmetric so it will have 2 real eigenvalues. We can find those eigenvalues by solving the characteristic equation
$$
\begin{align}
\text{det}\left|{
\begin{array}{cc}
1-\lambda & 1\\
-1 & 1-\lambda
\end{array}}\right| = 0\\
(1-\lambda)(1 - \lambda) - (1)(-1) = 0\\
1 - \lambda - \lambda + \lambda^2 - 1 = 0\\
\lambda^2 - 2\lambda = 0\\
\lambda(\lambda-2) = 0\\
\lambda = 0\text{ or }2
\end{align}
$$

There are two real eigenvalues as expected, $\lambda = 0$ and $\lambda = 2$. To find the eigenvectors, we solve the matrix equation $[A - \lambda I]\vec{x} = \vec{0}$ for each of the eigenvalues.

Since the first eigenvalue is $\lambda_1 = 0$, $A - 0I = A$. Therefore, the homogeneous system we have to solve to find the eigenvectors is
$$
\left[{
\begin{array}{ccc}
1 & 1 & 0\\
1 & 1 & 0
\end{array}}\right]
$$
Which gives the eigenvector
$$
\vec{x_1} = \left[{
\begin{array}{c}
-1\\
1
\end{array}}\right].
$$
For $\lambda_2 = 2$, we solve this homogeneous system
$$
\left[{
\begin{array}{ccc}
-1 & 1 & 0\\
1 & -1 & 0
\end{array}}\right]
$$
Which gives the eigenvector
$$
\vec{x_2} = \left[{
\begin{array}{c}
1\\
1
\end{array}}\right].
$$

### Problem 49
>Let
$$
\vec{x_1} = \left[{
\begin{array}{c}
1\\
4\\
1
\end{array}}\right], \vec{x_2} = \left[{
\begin{array}{c}
1\\
0\\
1
\end{array}}\right],
\vec{x_3} = \left[{
\begin{array}{c}
2\\
0\\
0
\end{array}}\right]
$$
Are these vectors linearly independent?

A quick inspection suggests that they are, but to know for sure, we can make them the column vectors of a matrix, $A$, and solve the homogeneous equation $A\vec{x} = \vec{0}$. If the only solution is the trivial solution, $\vec{x} = \vec{0}$, then the vectors are linearly independent.

```{r}
A <- matrix(c(1,1,2,
              4,0,0,
              1,1,0), ncol=3,nrow=3,byrow=TRUE)
Solve(A, c(0,0,0))

```
This is the case, so these vectors are linearly independent.

### Problem 50
> What is the rank of the matrix
$$
X = \left[{
\begin{array}{ccc}
1 & 1 & 2 \\
4 & 0 & 0 \\
1 & 1 & 0
\end{array}}\right]
$$
Is $X$ of full rank?

$X$ is full rank if the dimension of the column space or row space is 3. If there is a pivot in every column of $X$ then it will be full rank.
```{r}
X <- matrix(c(1,1,2,
              4,0,0,
              1,1,0), ncol=3, nrow=3, byrow=TRUE)

echelon(X)
```
We see that there is, in fact, a pivot in every row so this matrix is full rank.

### Problem 51
>Let
$$
\vec{x_1} = \left[{
\begin{array}{c}
1\\
1\\
2
\end{array}}\right], \vec{x_2} = \left[{
\begin{array}{c}
2\\
-3\\
-1
\end{array}}\right],
\vec{x_3} = \left[{
\begin{array}{c}
3\\
4\\
7
\end{array}}\right]
$$
Are these vectors linearly independent?

A quick inspection suggests that they are, but to know for sure, we can make them the column vectors of a matrix, $A$, and solve the homogeneous equation $A\vec{x} = \vec{0}$. If the only solution is the trivial solution, $\vec{x} = \vec{0}$, then the vectors are linearly independent.

```{r}
A <- matrix(c(1,2,3,
              1,-3,4,
              2,-1,7), ncol=3,nrow=3,byrow=TRUE)
Solve(A, c(0,0,0))

```
As it turns out, $x_3$ is a free variable and so these vectors are *not* linearly independent.

### Problem 52
What is the rank of the matrix
$$
Y = \left[{
\begin{array}{ccc}
1 & 2 & 3 \\
1 & -3 & 4\\
2 & -1 & 7
\end{array}}\right]
$$
Is $Y$ of full rank?

```{r}
Y <- matrix(c(1,2,3,
              1,-3,4,
              2,-1,7), ncol=3, nrow=3, byrow=TRUE)

echelon(Y)

```
There is a free variable, so this matrix is rank 2. It is not full rank.

### Problem 53
> Let $X$ be a $k\times k$ orthogonal matrix. Prove that $X$ is of full rank.

Since we know that $X$ is orthogonal, we know that the columns are linearly independent. This means that $X$ is nonsingular. The null space of $X$ has dimension zero and therefore, by the rank-nullity theorem, the rank must be $k$. This means that $X$ is full rank. We could have also just said that because the dimension of the column space is $k$, the matrix is full rank.

### Problem 54
> Suppose that $X$ is $k\times k$ or rank less than $k$. Is $X$ nonsingular? Explain.

Since we know that some of the columns of $X$ are linear combinations of the linearly independent ones, we know that the determinant of $X$ is 0. This means that the matrix is singular.

### Problem 55
> Consider the matrix $X$ of exercise 50. Is $X$ nonsingular? Explain.

This matrix is full rank, so we expect that its determinant is not zero. 

```{r}
X <- matrix(c(1,1,2,
              4,0,0,
              1,1,0), ncol=3, nrow=3, byrow=TRUE)

echelon(X)

det(X)

```
Computing the determinant show this to be the case, so $X$ is nonsingular as expected.

