---
title: "Chapter 2 - Quadratic Forms"
format: html
---

A quadratic form is a relation between two or more variables that only contains quadratic terms. For example, $ax^2 + bxy + cy^2$ is a quadratic form. We might express it slightly differently for convenience as $ax^2 + 2bxy + cy^2$ for reasons that will become clear shortly. The format is fine when we only have two terms, but it gets really cumbersome as more variables are related.

If we only had linear terms, we would have an expression like $ax + by +cz$ and we could express this as the dot product of two vectors:
$$
\left[{
\begin{array}{c}
a \\
b \\
c
\end{array}}\right]\cdot
\left[{
\begin{array}{c}
x \\
y \\
z
\end{array}}\right]
$$
and this, in turn, could be expressed more compactly as $\vec{v}\cdot\vec{x}$. We could add many more terms, but the expression remains the same.

So the question is, can we do the same for quadratic forms? It turns out we can! Since we have more dimensions, we need a matrix to encode the constant terms and we need a vector for the variables. Take the two dimensional case.
$$
M = \left[{
\begin{array}{cc}
a & b \\
b & c
\end{array}}\right]
$$
is a symmetric matrix and 
$$
\vec{x} = \left[{
\begin{array}{c}
x\\
y
\end{array}}\right]
$$
is a vector containing the variables $x$ and $y$. To capture the relationship $ax^2 + 2by + cy^2$, we perform matrix multiplication.
$$
M\vec{x} = \left[{
\begin{array}{cc}
a & b \\
b & c
\end{array}}\right]
\left[{
\begin{array}{c}
x\\
y
\end{array}}\right] = 
\left[{
\begin{array}{cc}
ax + by \\
bx + cy
\end{array}}\right]
$$
This gets us closer, but we don't yet have the quadratic form. We have a $1\times 2$ matrix, or a vector of terms and none are quadratic. To get the rest of the way, we need to introduce the final piece of the puzzle, $\vec{x}^T$. If we multiply $\vec{x}^TM$, we have
$$
\left[{
\begin{array}{cc}
x & y
\end{array}}\right]
\left[{
\begin{array}{cc}
ax + by \\
bx + cy
\end{array}}\right] = 
x(ax + by) + y(bx + cy) = ax^2 + 2bxy + cy^2
$$
This shows the result that we were after and why we used $2b$ as the coefficient in the original exposition of the two variable quadratic form.

$\vec{x}^TM\vec{x}$ remains compact regardless of the number of terms and variables we have. If we have 3 variables, the quadratic form would be $ax^2 + bxy _cyz + dxz + ey^2 + fz^2$ and that's just 3 variables! It's easier to express this with a $3\times 3$ symmetric matrix and a vector and its transpose.
$$
\left[{
\begin{array}{ccc}
x & y & z
\end{array}}\right]
\left[{
\begin{array}{cc}
a & b & c \\
b & d & e \\
c & e & f
\end{array}}\right]
\left[{
\begin{array}{c}
x\\
y\\
z
\end{array}}\right]
$$

As the number of terms grows, $\vec{x}^TM\vec{x}$ remains compact!

If we want to expand $\vec{x}^TM\vec{x} = q$, we can use summation notation
$$
q = \sum^k_{i=1}\sum^k_{j=1} m_{ij}x_ix_j
$$

Quadratic forms can be assigned to different categories. Two of those types are **positive definite** and **positive semidefinite**. $\vec{x}^TM\vec{x}$ is said to be positive definite if $\vec{x}^TM\vec{x} > 0$ for all $\vec{x} \neq \vec{0}$ and positive semidefinite if $\vec{x}^TM\vec{x} \geq 0$ for all $\vec{x}$ and $\vec{x}^TM\vec{x} = 0$ for some $\vec{x} \neq \vec{0}$.

**Theorem 2.1.1** - A symmetric matrix $A$ is positive definite if and only if all of its eigenvalues are positive.

**Theorem 2.1.2** - A symmetric matrix $A$ is positive semidefinite if and only if its eigenvalues are all nonnegative and at least one eigenvalue is zero.



```{r}
A <- matrix(c(1/sqrt(2),-1/sqrt(2),0,
              1/sqrt(2),1/sqrt(2),0,
              0,0,1), ncol=3, nrow=3, byrow=TRUE)

A_t <- t(A)

A_t %*% A

A %*% A_t


```







## Exercises

### Problem 27
> Let $Y_i$ and $Y_j$ be random variables with means $\mu_i$ and $\mu_j$, respectively. Then $\text{cov}(Y_i, Y_j) = E[Y_iY_j] - E[Y_i]E[Y_j]$. That is, $\sigma_{ij} = E[Y_iY_j] - \mu_i\mu_j$. Verify this result. *(Hint: Recall that $\text{cov}(Y_i, Y_j) = E[(Y_i - \mu_i)(Y_j - \mu_j)]$. Multiply $(Y_i - \mu_i)$ by $(Y_j - \mu_j)$ and simplify the resulting expression).* 

Key facts for this analysis:

1) The expectation of a constant is just that constant.
2) The expectation acts like a linear operator so $E[A + B] = E[A]+E[B]$ and $E[cA] = cE[A]$ where $c$ is a scalar.

We start out with $\text{cov}(Y_i, Y_j) = E[Y_iY_j] - E[Y_i]E[Y_j]$ and we are given that $E[Y_i]=\mu_i$ and $E[Y_j]=\mu_j$. We know that $\text{cov}(Y_i, Y_j) = E[(Y_i - \mu_i)(Y_j - \mu_j)]$ and if we multiply the terms we get $\text{cov}(Y_i,Y_j) = E[(Y_iY_j - \mu_jY_i - \mu_iY_j + \mu_i\mu_j)]$. We can use the two properties of the expectation listed above to simplify this expression.
$$
\begin{align}
E[(Y_iY_j - \mu_jY_i - \mu_iY_j + \mu_i\mu_j)] &= E[(Y_iY_j] - E[\mu_jY_i] - E[\mu_iY_j] + E[\mu_i\mu_j]\\
&=E[(Y_iY_j] - \mu_jE[Y_i] - \mu_iE[Y_j] + \mu_i\mu_j\\
&=E[(Y_iY_j] - \mu_i\mu_j - \mu_i\mu_j + \mu_i\mu_j\\
&=E[(Y_iY_j] - \mu_i\mu_j
\end{align}
$$

### Problem 28
> Let
$$
A = \left[{
\begin{array}{cc}
a_{11} & a_{12} & a_{13}\\
a_{21} & a_{22} & a_{23}\\
a_{31} & a_{32} & a_{33}\\
\end{array}}\right],\quad B = \left[{
\begin{array}{cc}
b_{11} & b_{12} & b_{13}\\
b_{21} & b_{22} & b_{23}\\
b_{31} & b_{32} & b_{33}\\
\end{array}}\right]
$$
Show that $tr(AB) = \sum_{i=1}^3 \sum_{j=1}^3 a_{ij}b_{ji}$.

The trace is the sum of the diagonal elements of $AB$ so we can just compute the matrix multiplication steps for the diagonal elements and compare them to the double sum.

$$
\begin{align}
ab_{11} = a_{11}b_{11} + a_{12}b_{21} + a_{13}b_{31}\\
ab_{11} = a_{21}b_{12} + a_{22}b_{22} + a_{23}b_{32}\\
ab_{31} = a_{11}b_{13} + a_{32}b_{23} + a_{33}b_{33}\\
\end{align}
$$
By inspection, these match up with the indexes on the entries in the double sum. The diagonals occur when the ith row of $A$ is dotted with the $ith$ column of $B$. We "walk" the columns of $A$ and the rows of $B$ at each step and that is why the indexes for $A$ are $ij$ and are $ji$ for $B$.

> Let $A$ and $B$ be $k \times k$ matrices. Show that $tr(AB) = \sum_{i=1}^k \sum_{j=1}^k a_{ij}b_{ji}$.

This is a simple extension of the the case in part a. We still produce the diagonal terms in the same way, there are now just $k$ of them rather than 3, but the mechanics are identical.

### Problem 35
>Let $P$ be an $n\times n$ matrix partitioned as $P = [P_1 | P_2]$ where$P_1$ is an $n\times k$ matrix and $P_2$ is an $n \times (n-k)$ matrix.
a) What are the dimensions of $P^T$? of $P_1^T$? of $P_2^T$? Show that
$$
P^T = \left[
\begin{array}{c}
P^T_1\\
\hline
P^T_2
\end{array}
\right]
$$
b) Let $\vec{y}$ be an $n\times 1$ vector. Show that
$$
P^T\vec{y} = \left[
\begin{array}{c}
P^T_1\vec{y}\\
\hline
P^T_2\vec{y}
\end{array}
\right]
$$
What are the dimensions of $P^T_1\vec{y}$ and $P^T_2\vec{y}$?
c) Let $A$ be an $n \times n$ matrix. Argue that
$$
P^TAP = \left[
\begin{array}{c|c}
P^T_1AP_1 & P^T_1AP_2\\
\hline
P^T_2AP_1 & P^T_2AP_2
\end{array}
\right]
$$
What are the dimensions of each of these submatrices?

