---
title: "Chapter 2 - Quadratic Forms"
format: html
---

A quadratic form is a relation between two or more variables that only contains quadratic terms. For example, $ax^2 + bxy + cy^2$ is a quadratic form. We might express it slightly differently for convenience as $ax^2 + 2bxy + cy^2$ for reasons that will become clear shortly. The format is fine when we only have two terms, but it gets really cumbersome as more variables are related.

If we only had linear terms, we would have an expression like $ax + by +cz$ and we could express this as the dot product of two vectors:
$$
\left[{
\begin{array}{c}
a \\
b \\
c
\end{array}}\right]\cdot
\left[{
\begin{array}{c}
x \\
y \\
z
\end{array}}\right]
$$
and this, in turn, could be expressed more compactly as $\vec{v}\cdot\vec{x}$. We could add many more terms, but the expression remains the same.

So the question is, can we do the same for quadratic forms? It turns out we can! Since we have more dimensions, we need a matrix to encode the constant terms and we need a vector for the variables. Take the two dimensional case.
$$
M = \left[{
\begin{array}{cc}
a & b \\
b & c
\end{array}}\right]
$$
is a symmetric matrix and 
$$
\vec{x} = \left[{
\begin{array}{c}
x\\
y
\end{array}}\right]
$$
is a vector containing the variables $x$ and $y$. To capture the relationship $ax^2 + 2by + cy^2$, we perform matrix multiplication.
$$
M\vec{x} = \left[{
\begin{array}{cc}
a & b \\
b & c
\end{array}}\right]
\left[{
\begin{array}{c}
x\\
y
\end{array}}\right] = 
\left[{
\begin{array}{cc}
ax + by \\
bx + cy
\end{array}}\right]
$$
This gets us closer, but we don't yet have the quadratic form. We have a $1\times 2$ matrix, or a vector of terms and none are quadratic. To get the rest of the way, we need to introduce the final piece of the puzzle, $\vec{x}^T$. If we multiply $\vec{x}^TM$, we have
$$
\left[{
\begin{array}{cc}
x & y
\end{array}}\right]
\left[{
\begin{array}{cc}
ax + by \\
bx + cy
\end{array}}\right] = 
x(ax + by) + y(bx + cy) = ax^2 + 2bxy + cy^2
$$
This shows the result that we were after and why we used $2b$ as the coefficient in the original exposition of the two variable quadratic form.

$\vec{x}^TM\vec{x}$ remains compact regardless of the number of terms and variables we have. If we have 3 variables, the quadratic form would be $ax^2 + bxy _cyz + dxz + ey^2 + fz^2$ and that's just 3 variables! It's easier to express this with a $3\times 3$ symmetric matrix and a vector and its transpose.
$$
\left[{
\begin{array}{ccc}
x & y & z
\end{array}}\right]
\left[{
\begin{array}{cc}
a & b & c \\
b & d & e \\
c & e & f
\end{array}}\right]
\left[{
\begin{array}{c}
x\\
y\\
z
\end{array}}\right]
$$

As the number of terms grows, $\vec{x}^TM\vec{x}$ remains compact!

If we want to expand $\vec{x}^TM\vec{x} = q$, we can use summation notation
$$
q = \sum^k_{i=1}\sum^k_{j=1} m_{ij}x_ix_j
$$

Quadratic forms can be assigned to different categories. Two of those types are **positive definite** and **positive semidefinite**. $\vec{x}^TM\vec{x}$ is said to be positive definite if $\vec{x}^TM\vec{x} > 0$ for all $\vec{x} \neq \vec{0}$ and positive semidefinite if $\vec{x}^TM\vec{x} \geq 0$ for all $\vec{x}$ and $\vec{x}^TM\vec{x} = 0$ for some $\vec{x} \neq \vec{0}$.

**Theorem 2.1.1** - A symmetric matrix $A$ is positive definite if and only if all of its eigenvalues are positive.

**Theorem 2.1.2** - A symmetric matrix $A$ is positive semidefinite if and only if its eigenvalues are all nonnegative and at least one eigenvalue is zero.



```{r}
A <- matrix(c(1/sqrt(2),-1/sqrt(2),0,
              1/sqrt(2),1/sqrt(2),0,
              0,0,1), ncol=3, nrow=3, byrow=TRUE)

A_t <- t(A)

A_t %*% A

A %*% A_t


```


## Exercises

### Problem 1
> Show that $\vec{y}A\vec{y} = \sum_{i=1}^k\sum_{j=1}^k a{ij}y_iy_j$ by writing $y^T$ as $[y_1\quad y_2\quad\cdots y_n]$ and writing $A$ in the form
$$
\left[{
\begin{array}{cc}
a_{11} & a_{12} & \cdots & a_{1k} \\
a_{21} & a_{22} & \cdots & a_{2k} \\
\vdots & \vdots &  & \vdots \\
a_{k1} & a_{k2} &  & a_{kk}  
\end{array}}\right]
$$
and multiplying.

We start by multiplying $\vec{y}^T$ and $A$.

$$
[y_1\quad y_2\quad\cdots\quad y_n]\left[{
\begin{array}{cc}
a_{11} & a_{12} & \cdots & a_{1k} \\
a_{21} & a_{22} & \cdots & a_{2k} \\
\vdots & \vdots &  & \vdots \\
a_{k1} & a_{k2} &  & a_{kk}  
\end{array}}\right]
$$
This product generates the following columns of the $1\times k$ matrix:
$$
\begin{gather}
y_1a_{11} + y_2a_{21} + \cdots + y_ka_{k1}\\
y_1a_{12} + y_2a_{22} + \cdots + y_ka_{k2}\\
y_1a_{13} + y_2a_{23} + \cdots + y_ka_{k3}\\
\vdots\\
y_1a_{1k} + y_2a_{2k} + \cdots + y_ka_{kk}
\end{gather}
$$
We then take the dot product of this result and $\vec{y}$ which yields the following, massive scalar:
$$
\begin{gather}
y_1^2a_{11} + y_2a_{21} + \cdots + y_1y_ka_{k1}\\
y_1y_2a_{12} + y_2^2a_{22} + \cdots + y_2y_ka_{k2}\\
\vdots\\
y_1y_ka_{1k} + y_2y_ka_{2k} + \cdots + y_k^2a_{kk}
\end{gather}
$$
If we organise the individual elements of this sum, we can see that they fit the pattern of a double incrementing loop that demonstrates that $\vec{y}A\vec{y} = \sum_{i=1}^k\sum_{j=1}^k a{ij}y_iy_j$.

### Problem 2
>Let
$$
\vec{y} = \left[{
\begin{array}{c}
y_1 \\
y_2 \\
\end{array}}\right]\quad\text{ and }\quad A = \left[{
\begin{array}{cc}
2 & 4 \\
1 & 6 \\
\end{array}}\right]
$$
Find $\vec{y}^TA\vec{y}$ via the addition formula of Exercise 1 and by direct matrix multiplication.

Applying the addition formula, we have
$$\begin{align}
\sum_{i=1}^2\sum_{j=1}^2 a_{ij}y_iy_j &= a_{11}y_i^2 + a_{12}y_1y_2 + a_{21}y_1y_2 + a_{22}y_2^2\\
&= 2y_1^2 + 4y_1y_2 + y_1y_2 + 6y_2^2\\
&= 2y_1^2 + 5y_1y_2 + 6y_2^2
\end{align}
$$
Now, by direct matrix multiplication
$$
\begin{align}
\left[{
\begin{array}{c}
y_1 & y_2 \\
\end{array}}\right]\left[{
\begin{array}{cc}
2 & 4 \\
1 & 6 \\
\end{array}}\right] &= \left[{
\begin{array}{c}
2y_1 + y_2 & 4y_1 + 6y_2 \\
\end{array}}\right]\\
\left[{
\begin{array}{c}
2y_1 + y_2 & 4y_1 + 6y_2 \\
\end{array}}\right]\left[{
\begin{array}{cc}
y_1\\
y_2\\
\end{array}}\right] &= 2y_1^2 + y_1y_2 + 4y_1y_2 + 6y_2^2\\
&= 2y_1^2 + 5y_1y_2 + 6y_2^2
\end{align}
$$
So we see the same result either way.

### Problem 3
> Let
$$
\vec{y} = \left[{
\begin{array}{c}
y_1 \\
y_2 \\
y_3
\end{array}}\right]\quad\text{ and }\quad A = \left[{
\begin{array}{ccc}
1 & 2 & 0\\
0 & 1 & -1\\
1 & 3 & 6\\
\end{array}}\right]
$$
Find $\vec{y}^TA\vec{y}$ via the addition formula of Exercise 1 and by direct matrix multiplication. Do you notice any pattern to the coefficients that would allow you to form $\vec{y}^TA\vec{y}$ by inspection alone?

Applying the addition formula, we have
$$\begin{align}
\sum_{i=1}^3\sum_{j=1}^3 a_{ij}y_iy_j &= a_{11}y_i^2 + a_{12}y_1y_2 + a_{13}y_i^2 + a_{21}y_1y_2 + a_{22}y_2^2 + a_{23}y_i^2 + a_{31}y_1y_2 + a_{32}y_2^2 + a_{33}y_i^2\\
&= 1y_1^2 + 1y_2^2 + 6y_3^2 + 2y_1y_2 + 1y_1y_3 + 2y_2y_3
\end{align}
$$

Now, by direct matrix multiplication
$$
\begin{align}
\left[{
\begin{array}{c}
y_1 & y_2 & y_3 \\
\end{array}}\right]\left[{
\begin{array}{cc}
1 & 2 & 0\\
0 & 1 & -1\\
1 & 3 & 6\\
\end{array}}\right] &= \left[{
\begin{array}{c}
y_1 + y_3 & 2y_1 + y_2 + 3y_3 & -y_2 + 6y_3 \\
\end{array}}\right]\\
\left[{
\begin{array}{c}
y_1 + y_3 & 2y_1 + y_2 + 3y_3 & -y_2 + 6y_3 \\
\end{array}}\right]\left[{
\begin{array}{cc}
y_1\\
y_2\\
y_3
\end{array}}\right] &= y_1^2 + y_1y_3 + 2y_1y_2 + y_2^2 + 3y_2y_3 - 1y_2y_3 + 6y_3^2\\
&= y_1^2 + y_2^2 + 6y_3^2 + 2y_1y_2 + y_1y_3 + 2y_2y_3
\end{align}
$$

The 2nd degree terms have coefficients equal to the diagonal of $A$. The cross terms have coefficients equal to the sum of the cross terms in $A$. For example, $y_1y_3$ has coefficient equal to the sum of $a_{13} + a_{31} = 0+1 = 1$.

### Problem 4
> Let
$$
\vec{y} = \left[{
\begin{array}{ccc}
y_1 \\
y_2 \\
y_3\\y_4
\end{array}}\right]\quad\text{ and }\quad A = \left[{
\begin{array}{cccc}
1 & 4 & 8 & 0 \\
0 & 6 & 1 & 1 \\
3 & 1 & 2 & 4\\
1 & -1 & 2 & 3
\end{array}}\right]
$$
Without doing any computations, what is the coefficient or the term $y_3^2$ in the expression for $\vec{y}^TA\vec{y}$? What is the coefficient for $y_1y_3$ in this expression? For $y_2y_3$?

For $y_3^2$, the coefficient will be $a_{33}$ which is 2. For $y_1y_3$ the coefficient will be the sum of the off-diagonal terms $a_{13} + a_{31} = 8+3 = 11$. For $y_2y_3$ the coefficient will be the sum of the off-diagonal terms $a_{23} + a_{32} = 1+1 = 2$.

### Problem 5
> Consider the matrix
$$
A = \left[{
\begin{array}{cc}
2 & -1 \\
-1 & 2\\
\end{array}}\right]
$$
Complete the argument that $A$ is positive definite by showing that if $y_1\leq y_2 \leq 0$, then $2y_1^2 + 2y_2^2 - 2y_1y_2 > 0$.

We know that the squared terms are always positive. But $y_1y_2 \geq 0$ but $y_1y_2 \leq y_1^2$ so when we perform the subtraction, we know that $y_1^2 - y_1y_2 \geq 0$ which makes the inequality true for any $\vec{y}\neq \vec{0}$.

### Problem 6
> Let
$$
A = \left[{
\begin{array}{cc}
2 & 1 \\
1 & 2\\
\end{array}}\right]
$$
Show that $A$ is positive definite via Definition 2.1.2; verify you answer by applying Theorem 2.1.1 to this matrix.

$2y_1^2 + 2y_2^2 + y_1y_2 > 0$ is always true if $y_2 \geq y_2 \geq 0$ or if $y_1 \leq y_2 \leq 0$. If $y_1$ or $y_2$ is negative, we have to evaluate the cross term. This product will always be smaller than the square of the larger (in magnitude) term and so this expression will always be positive making $A$ a positive definite matrix.

To apply Theorem 2.1.1, we need to compute the eigenvalues of $A$.

The characteristic equation for this matrix comes from $\text{det}(A - \lambda I) = 0$.
$$
(2 - \lambda)(2 - \lambda) - 1 = 0\\
4 - 3\lambda + \lambda^2 - 1 = 0\\
\lambda^2 - 4\lambda + 3 = 0\\
(\lambda-3)(\lambda-1) = 0\\
\lambda = 1\text{ or }3
$$
Both values are positive confirming that this matrix is positive definite.

### Problem 7
>Let
$$
A = \left[{
\begin{array}{cc}
1 & 1 \\
1 & 2 \\
\end{array}}\right]
$$
Is $A$ positive definite, positive semidefinite, or neither?

To answer this, we just need to determine the value of the eigenvalues of $A$ and apply Theorem 2.1.1 or Theorem 2.1.2.

The characteristic equation for this matrix comes from $\text{det}(A - \lambda I) = 0$.

$$
(1 - \lambda)(2 - \lambda) - 1 = 0\\
2 - 3\lambda + \lambda^2 - 1 = 0\\
\lambda^2 - 3\lambda + 1 = 0\\
\lambda = \frac{3 \pm \sqrt{5}}{2}
$$
Both values are positive and so this matrix is positive definite.

### Problem 8
>Let
$$
A = \left[{
\begin{array}{cc}
2 & 2 \\
2 & 2 \\
\end{array}}\right]
$$
Is $A$ positive definite, positive semidefinite, or neither?

The characteristic equation for $A$ is $\text{det}(A - \lambda I) = 0$.

$$
(2 - \lambda)(2 - \lambda) - 4 = 0\\
4 - 4\lambda + \lambda^2 - 4 = 0\\
\lambda^2 - 4\lambda = 0\\
\lambda(\lambda - 4) = 0\\
\lambda = 0\text{ or }4
$$
Since one eigenvalue is zero and the other is positive, $A$ is a positive semidefinite matrix.

### Problem 9
>Let
$$
A = \left[{
\begin{array}{cc}
0 & -1 \\
-1 & 0 \\
\end{array}}\right]
$$
Is $A$ positive definite, positive semidefinite, or neither?

The characteristic equation for $A$ is $\text{det}(A - \lambda I) = 0$.

$$
(0 - \lambda)(0 - \lambda) - 1 = 0\\
\lambda^2 - 1 = 0\\
\lambda^2 = 1\\
\lambda = \pm 1
$$
Since one of the eigenvalues of $A$ is negative, this matrix is neither positive definite or positive semidefinite.


### Problem 10
> Assume that $A$ is a symmetric matrix whose eigenvalues are all positive. Show that $A$ is positive definite, thus partially proving Theorem 2.1.1. Let $\vec{y}$ denote a nonzero vector. Write $\vec{y}^TA\vec{y}$ as $\vec{y}P(P^TAP)P^T\vec{y} = \vec{z}(P^TAP)\vec{z}$ where $P$ is the orthogonal matrix that diagonalises $A$ guaranteed in Theorem 1.4.1. Argue that if all eigenvalues are positive then $\vec{z}^T(P^TAP)\vec{z}>0$ by expanding the quadratic form with $P^TAP$ in diagonal form.

Let
$$
\Lambda =  \left[{
\begin{array}{cc}
\lambda_1 & 0 & \cdots & 0\\
0 & \lambda_2 & \cdots & 0\\
0 & 0 & \ddots & 0\\
0 & 0 & \cdots & \lambda_n\\
\end{array}}\right]
$$
be the diagonal matrix of eigenvalues. Recall that $\Lambda = P^TAP$. Therefore, we need to show that $\vec{z}^T\Lambda\vec{z}>0$ by expanding the matrix multiplication.

$$
\vec{z}^T\Lambda = \left[{
\begin{array}{ccc}
z_1 & z_2 & \cdots & z_n\\
\end{array}}\right]\left[{
\begin{array}{cc}
\lambda_1 & 0 & \cdots & 0\\
0 & \lambda_2 & \cdots & 0\\
0 & 0 & \ddots & 0\\
0 & 0 & \cdots & \lambda_n\\
\end{array}}\right] = \left[{
\begin{array}{ccc}
z_1\lambda_1 & z_2\lambda_2 & \cdots & z_n\lambda_n
\end{array}}\right]
$$
$$
\left[{
\begin{array}{ccc}
z_1\lambda_1 & z_2\lambda_2 & \cdots & z_n\lambda_n
\end{array}}\right]\left[{
\begin{array}{cc}
z_1\\
z_2\\
\cdots\\
z_n
\end{array}}\right] = z_1^2\lambda_1 + z_2^2\lambda_2 + \cdots + z_n^2\lambda_n
$$
Since $\lambda_i > 0$ for all $i$ and $z_i^2>0$ for all $z_i$, then $\vec{z}^T(P^TAP)\vec{z}>0$ must be true for all $\vec{z}\neq0$.

### Problem 11
>Let
$$
\vec{a} = \left[{
\begin{array}{cc}
8\\
3\\
2
\end{array}}\right]\text{ and }
\vec{y} = \left[{
\begin{array}{cc}
y_1\\
y_2\\
y_3
\end{array}}\right]
$$
Write $\vec{z} = \vec{a}^T\vec{y}$ in expanded form and find $\partial z/\partial y_1$, $\partial z/\partial y_2$, and $\partial z/\partial y_3$.

$$
\vec{z} = \vec{a}^T\vec{y} = \left[{
\begin{array}{cc}
8&3&2
\end{array}}\right]\left[{
\begin{array}{cc}
y_1\\
y_2\\
y_3
\end{array}}\right] = 8y_1 + 3y_2 + 2y_3
$$
Taking the partial derivatives of this expression gives us
$$
\begin{align}
\partial z/\partial y_1 = 8\quad\partial z/\partial y_2&=3\quad\partial z/\partial y_3 = 2\\[0.25cm]
\partial \vec{z}/\partial \vec{y} &= \left[{
\begin{array}{cc}
8\\
3\\
2
\end{array}}\right]
\end{align}
$$

### Problem 12
>Let $\vec{y}$ be a $5\times 1$ column vector and let $z=\vec{y}^T\vec{y}$.
a) Express $z$ in expanded form and find $\partial z/\partial y_i$ for $i=1,2,3,4,5$.
b) Verify that $\partial z/\partial \vec{y} = 2\vec{y}$ as claimed in rule 2 for derivatives.

a) $z = \vec{y}^T\vec{y}$.
$$
z = \vec{y}^T\vec{y} = \left[{
\begin{array}{cc}
y_1\\
y_2\\
y_3\\
y_4\\
y_5\\
\end{array}}\right]
\left[{
\begin{array}{cc}
y_1&
y_2&
y_3&
y_4&
y_5
\end{array}}\right] = y_1^2 + y_2^2 + y_3^2 + y_4^2 + y_5^2
$$
Now taking partial derivatives we get

$$
\partial z/\partial y_1 = 2y_1\quad\partial z/\partial y_2=2y_2\quad\partial z/\partial y_3 = 2y_3\quad\partial z/\partial y_4 = 2y_4\quad\partial z/\partial y_5 = 2y_5
$$

Now confirm that $\quad\partial z/\partial y = 2\vec{y}$.

$$
\frac{\partial z}{\partial \vec{y}} = \left[{
\begin{array}{cc}
2y_1\\
2y_2\\
2y_3\\
2y_4\\
2y_5
\end{array}}\right] = 2\left[{
\begin{array}{cc}
y_1\\
y_2\\
y_3\\
y_4\\
y_5
\end{array}}\right] = 2\vec{y}
$$

### Problem 13
>Let
$$
\vec{y} = \left[{
\begin{array}{cc}
y_1\\
y_2\\
y_3
\end{array}}\right]
$$
and let $z=\vec{y}^T\vec{y}$. Suppose that when $\partial z/\partial \vec{y}$ is evaluated at the point $\vec{y_o}$, we obtain
$$
\partial z/\partial \vec{y} = \left[{
\begin{array}{cc}
6\\
-4\\
10
\end{array}}\right]
$$
Find $\vec{y_0}$.

Recall that $\partial z/\partial \vec{y} = 2\vec{y}$, so $2y_1 = 6,\text{ }2y_2 = -4,\text{ }2y_3 = 10$ and so $y_1 = 3,\text{ }y_2 = -2,\text{ }y_3 = 5$. 
Therefore,
$$
\vec{y_0} = \left[{
\begin{array}{cc}
3\\
-2\\
5
\end{array}}\right]
$$

### Problem 14
> Let
$$
A = \left[{
\begin{array}{cc}
2 & 3\\
1 & 6\\
\end{array}}\right]
$$
and let $z = \vec{y}^TA\vec{y}$. Find $\partial z/\partial \vec{y}$ using rule 3 for derivatives, and check your answer by direct differentiation.

Rule 3 states that $\partial z/\partial \vec{y} = A\vec{y} + A^T\vec{y}$.
$$
A^T = \left[{
\begin{array}{cc}
2 & 1\\
3 & 6\\
\end{array}}\right]
$$
Therefore,
$$
\partial z/\partial \vec{y} = A\vec{y} + A^T\vec{y} = \left[{
\begin{array}{cc}
2 & 3\\
1 & 6\\
\end{array}}\right]\left[{
\begin{array}{cc}
y_1\\
y_2
\end{array}}\right] + \left[{
\begin{array}{cc}
2 & 1\\
3 & 6\\
\end{array}}\right]\left[{
\begin{array}{cc}
y_1\\
y_2
\end{array}}\right]
$$

$$
A\vec{y} = \left[{
\begin{array}{cc}
2 & 3\\
1 & 6\\
\end{array}}\right]\left[{
\begin{array}{cc}
y_1\\
y_2
\end{array}}\right] = \left[{
\begin{array}{cc}
2y_1 + 3y_2\\
y_1 + 6y_2
\end{array}}\right]
$$
$$
A^T\vec{y} = \left[{
\begin{array}{cc}
2 & 1\\
3 & 6\\
\end{array}}\right]\left[{
\begin{array}{cc}
y_1\\
y_2
\end{array}}\right] = \left[{
\begin{array}{cc}
2y_1 + y_2\\
3y_1 + 6y_2
\end{array}}\right]
$$
$$
A\vec{y} + A^T\vec{y} = \left[{
\begin{array}{cc}
2y_1 + 3y_2\\
y_1 + 6y_2
\end{array}}\right] + \left[{
\begin{array}{cc}
2y_1 + y_2\\
3y_1 + 6y_2
\end{array}}\right] = \left[{
\begin{array}{cc}
4y_1 + 4y_2\\
4y_1 + 12y_2
\end{array}}\right]
$$

Now, we can use the direct method to verify.
$$
\begin{align}
z = \vec{y}^TA\vec{y} = 2y_1^2 + 6y_2^2 + 4y_1y_2\\[0.25cm]
\frac{\partial z}{\partial\vec{y}} = 
\left[{
\begin{array}{cc}
\partial z/\partial y_1\\
\partial z/\partial y_2
\end{array}}\right] = \left[{
\begin{array}{cc}
4y_1 + 4y_2\\
4y_1 + 12y_2
\end{array}}\right]
\end{align}
$$
The two results match up!

### Problem 15
> Let
$$
A = \left[{
\begin{array}{cc}
2 & 6\\
4 & -3\\
\end{array}}\right]\text{ and }A^T = \left[{
\begin{array}{cc}
2 & 4\\
6 & -3\\
\end{array}}\right]
$$
and let $z = \vec{y}^TA\vec{y}$. Find $\partial z/\partial \vec{y}$.

Recall that $\partial z/\partial \vec{y} = A\vec{y} + A^T\vec{y}$.

$$
\begin{align}
A\vec{y} = \left[{
\begin{array}{cc}
2 & 6\\
4 & -3\\
\end{array}}\right]\left[{
\begin{array}{cc}
y_1\\
y_2
\end{array}}\right] &= \left[{
\begin{array}{cc}
2y_1 + 6y_2\\
4y_1 - 3y_2
\end{array}}\right]\\[0.25cm]

A^T\vec{y} = \left[{
\begin{array}{cc}
2 & 4\\
6 & -3\\
\end{array}}\right]\left[{
\begin{array}{cc}
y_1\\
y_2
\end{array}}\right] &= \left[{
\begin{array}{cc}
2y_1 + 4y_2\\
6y_1 - 3y_2
\end{array}}\right]\\[0.25cm]
A\vec{y} + A^T\vec{y} = \left[{
\begin{array}{cc}
2y_1 + 6y_2\\
4y_1 - 3y_2
\end{array}}\right] + \left[{
\begin{array}{cc}
2y_1 + 4y_2\\
6y_1 - 3y_2
\end{array}}\right] &= \left[{
\begin{array}{cc}
4y_1 + 10y_2\\
10y_1 - 6y_2
\end{array}}\right]
\end{align}
$$

Thus
$$
\partial z/\partial \vec{y} = \left[{
\begin{array}{cc}
4y_1 + 10y_2\\
10y_1 - 6y_2
\end{array}}\right]
$$

### Problem 16
> Let
$$
A = \left[{
\begin{array}{ccc}
3 & 1 & 8\\
1 & 0 & 1\\
2 & 1 & -4
\end{array}}\right]
$$
and let $z = \vec{y}^TA\vec{y}$. Find $\partial z/\partial \vec{y}$.

Instead of doing the matrix multiplication, we'll used the shortcut of the diagonals being the coefficients for the squared terms and the sum of the off-diagonals being the coefficients for the cross terms. Doing this, we get the following quadratic equation for $z$.
$$
\begin{align}
z &= 3y_1^2 + 0y_2^2 - 4y_3^2 + 2y_1y_2 + 10y_1y_3 + 2y_2y_3\\
z &= 3y_1^2 - 4y_3^2 + 2y_1y_2 + 10y_1y_3 + 2y_2y_3
\end{align}
$$
Now we can just take the partials with respect to $y_1, y_2$, and $y_3$ and assemble $\partial z/\partial \vec{y}$.
$$
\begin{align}
\partial z/\partial y_1 &= 6y_1 + 2y_2 + 10y_3\\[0.25cm]
\partial z/\partial y_2 &= 2y_1 + 2y_3\\[0.25cm]
\partial z/\partial y_3 &= 10y_1 + 2y_2 - 8y_3\\[0.25cm]
\partial z/\partial \vec{y} &= \left[{
\begin{array}{ccc}
6y_1 + 2y_2 + 10y_3\\
2y_1 + 2y_3\\
10y_1 + 2y_2 - 8y_3
\end{array}}\right]
\end{align}
$$

### Problem 17
> Let
$$
A = \left[{
\begin{array}{ccc}
2 & 1 & 4\\
1 & 5 & 3\\
4 & 3 & -1
\end{array}}\right]
$$
and let $z = \vec{y}^TA\vec{y}$. Find $\partial z/\partial \vec{y}$. Do you notice anything unusual about the answer in this case? What property of $A$ causes this unusual result?

$\partial z/\partial \vec{y} = A\vec{y} + A^T\vec{y}$. Since $A$ is symmetric, $A = A^T$, and therefore $\partial z/\partial \vec{y} = A\vec{y} + A\vec{y} = 2A\vec{y}$.

By inspection of $A$, we can see that $z = 2y_1^2 + 5y_2^2 - y^3_2 + 2y_1y_2 + 8y_1y_3 + 6y_2y_3$. Taking partials, we get
$$
\frac{\partial z}{\partial \vec{y}} = \left[{
\begin{array}{ccc}
4y_1 + 2y_2 + 8y_3\\
2y_1 + 10y_2 + 6y_3\\
8y_1 + 6y_2 - 2y_3
\end{array}}\right]
$$

Now, compute $A\vec{y}$
$$
A\vec{y} = \left[{
\begin{array}{ccc}
2 & 1 & 4\\
1 & 5 & 3\\
4 & 3 & -1
\end{array}}\right] = \left[{
\begin{array}{ccc}
2y_1 + y_2 + 4y_3\\
y_1 + 5y_2 + 3y_3\\
4y_1 + 3y_2 - y_3
\end{array}}\right]
$$

Now, multiply this result by 2.

$$
2\left[{
\begin{array}{ccc}
2y_1 + y_2 + 4y_3\\
y_1 + 5y_2 + 3y_3\\
4y_1 + 3y_2 - y_3
\end{array}}\right] = \left[{
\begin{array}{ccc}
4y_1 + 2y_2 + 8y_3\\
2y_1 + 10y_2 + 6y_3\\
8y_1 + 6y_2 - 2y_3
\end{array}}\right]
$$
So the symmetry of $A$ is what makes the result somewhat unusual and makes things a little easier computationally.

### Problem 18
> Show that if $A$ is symmetric and $z = \vec{y}^TA\vec{y}$, then $\partial z/\partial \vec{y} = 2A\vec{y}$.

We already know that $\partial z/\partial \vec{y} = A\vec{y} + A^T\vec{y}$ and since A is assumed to be symmetric, $A = A^T$. Therefore, $\partial z/\partial \vec{y} = A\vec{y} + A^T\vec{y} = A\vec{y} + A\vec{y} = 2A\vec{y}$.
### Problem 19
Let
$$
Y = \left[{
\begin{array}{ccc}
Y_1\\
Y_2\\
Y_3
\end{array}}\right],\quad \vec{\mu} = \left[{
\begin{array}{ccc}
3\\
7\\
1
\end{array}}\right],\text{ and }
\vec{a} = \left[{
\begin{array}{ccc}
-1\\
2\\
1
\end{array}}\right]
$$
Find $E[a^TY]$.

$E[\vec{a}^TY] = \vec{a^T}E[Y]$. We know that $E[Y] = \vec{u}$, so $\vec{a^T}E[Y] = \vec{a^T}\vec{\mu}$. 

$$
\vec{a^T}\vec{\mu} = \left[{
\begin{array}{ccc}
-1 & 2 & 1
\end{array}}\right]\left[{
\begin{array}{ccc}
3\\
7\\
1
\end{array}}\right] = -3 + 14 + 1 = 12
$$


### Problem 20



### Problem 21



### Problem 22
> Let $Y$ be a random vector with $E[Y] = \mu$. Use the rules for expectation to show that $var Y = E[Y^TY] - \vec{\mu}\vec{\mu}^T$.

Recall that $var Y = E[(\vec{Y} - \vec{\mu})^2] = E[\vec{Y}\vec{Y}^T - \vec{Y}\vec{\mu}^T - \vec{\mu}\vec{Y}^T + \vec{\mu}\vec{\mu}^T]$.

We can distribute the expectation operator, which gives us
$$
E[\vec{Y}\vec{Y}^T - \vec{Y}\vec{\mu}^T - \vec{\mu}\vec{Y}^T + \vec{\mu}\vec{\mu}^T] = E[\vec{Y}\vec{Y}^T] - E[\vec{Y}\vec{\mu}^T] - E[\vec{\mu}\vec{Y}^T] + E[\vec{\mu}\vec{\mu}^T]
$$
Also recall that $E[\vec{Y}] = \vec{\mu}$ and that we can pull the constant $\vec{\mu}$ out of the expectation operation. Knowing these facts allows us to rewrite the above as
$$
E[\vec{Y}\vec{Y}^T] - E[\vec{Y}\vec{\mu}^T] - E[\vec{\mu}\vec{Y}^T] + E[\vec{\mu}\vec{\mu}^T] = E[\vec{Y}\vec{Y}^T] - \vec{\mu}\vec{\mu}^T - \vec{\mu}\vec{\mu}^T + \vec{\mu}\vec{\mu}^T
$$

We can then simplify this expression to $E[\vec{Y}\vec{Y}^T] - \vec{\mu}\vec{\mu}^T - \vec{\mu}\vec{\mu}^T + \vec{\mu}\vec{\mu}^T = E[\vec{Y}\vec{Y}^T] - \vec{\mu}\vec{\mu}^T$, which is what we were asked to demonstrate.

### Problem 25


### Problem 26
> The technical definition of a random sample of size $n$ is "a collection of $n$ independent and identically distributed random variables". Suppose that the random vector
$$
\left[{
\begin{array}{ccc}
Y_1\\
Y_2\\
Y_3
\end{array}}\right]
$$
represents a random sample of size 3 from a distribution with a variance of 6. What is the variance-covariance matrix for $\vec{Y}$?

$$
\text{var }\vec{Y} = V = \left[{
\begin{array}{ccc}
6 & 0 & 0\\
0 & 6 & 0\\
0 & 0 & 6
\end{array}}\right]
$$
We know that the random variables are assumed to be independent and thus their covariances should all be 0. The variances are on the diagonal and we are told that the variance is 6. Since the assumption is that the variables are identically distributed, they all have the same variance.

### Problem 27
> Let $Y_i$ and $Y_j$ be random variables with means $\mu_i$ and $\mu_j$, respectively. Then $\text{cov}(Y_i, Y_j) = E[Y_iY_j] - E[Y_i]E[Y_j]$. That is, $\sigma_{ij} = E[Y_iY_j] - \mu_i\mu_j$. Verify this result. *(Hint: Recall that $\text{cov}(Y_i, Y_j) = E[(Y_i - \mu_i)(Y_j - \mu_j)]$. Multiply $(Y_i - \mu_i)$ by $(Y_j - \mu_j)$ and simplify the resulting expression).* 

Key facts for this analysis:

1) The expectation of a constant is just that constant.
2) The expectation acts like a linear operator so $E[A + B] = E[A]+E[B]$ and $E[cA] = cE[A]$ where $c$ is a scalar.

We start out with $\text{cov}(Y_i, Y_j) = E[Y_iY_j] - E[Y_i]E[Y_j]$ and we are given that $E[Y_i]=\mu_i$ and $E[Y_j]=\mu_j$. We know that $\text{cov}(Y_i, Y_j) = E[(Y_i - \mu_i)(Y_j - \mu_j)]$ and if we multiply the terms we get $\text{cov}(Y_i,Y_j) = E[(Y_iY_j - \mu_jY_i - \mu_iY_j + \mu_i\mu_j)]$. We can use the two properties of the expectation listed above to simplify this expression.
$$
\begin{align}
E[(Y_iY_j - \mu_jY_i - \mu_iY_j + \mu_i\mu_j)] &= E[(Y_iY_j] - E[\mu_jY_i] - E[\mu_iY_j] + E[\mu_i\mu_j]\\
&=E[(Y_iY_j] - \mu_jE[Y_i] - \mu_iE[Y_j] + \mu_i\mu_j\\
&=E[(Y_iY_j] - \mu_i\mu_j - \mu_i\mu_j + \mu_i\mu_j\\
&=E[(Y_iY_j] - \mu_i\mu_j
\end{align}
$$

### Problem 28
> Let
$$
A = \left[{
\begin{array}{cc}
a_{11} & a_{12} & a_{13}\\
a_{21} & a_{22} & a_{23}\\
a_{31} & a_{32} & a_{33}\\
\end{array}}\right],\quad B = \left[{
\begin{array}{cc}
b_{11} & b_{12} & b_{13}\\
b_{21} & b_{22} & b_{23}\\
b_{31} & b_{32} & b_{33}\\
\end{array}}\right]
$$
Show that $tr(AB) = \sum_{i=1}^3 \sum_{j=1}^3 a_{ij}b_{ji}$.

The trace is the sum of the diagonal elements of $AB$ so we can just compute the matrix multiplication steps for the diagonal elements and compare them to the double sum.

$$
\begin{align}
ab_{11} = a_{11}b_{11} + a_{12}b_{21} + a_{13}b_{31}\\
ab_{11} = a_{21}b_{12} + a_{22}b_{22} + a_{23}b_{32}\\
ab_{31} = a_{11}b_{13} + a_{32}b_{23} + a_{33}b_{33}\\
\end{align}
$$
By inspection, these match up with the indexes on the entries in the double sum. The diagonals occur when the ith row of $A$ is dotted with the $ith$ column of $B$. We "walk" the columns of $A$ and the rows of $B$ at each step and that is why the indexes for $A$ are $ij$ and are $ji$ for $B$.

> Let $A$ and $B$ be $k \times k$ matrices. Show that $tr(AB) = \sum_{i=1}^k \sum_{j=1}^k a_{ij}b_{ji}$.

This is a simple extension of the the case in part a. We still produce the diagonal terms in the same way, there are now just $k$ of them rather than 3, but the mechanics are identical.

### Problem 35
>Let $P$ be an $n\times n$ matrix partitioned as $P = [P_1 | P_2]$ where$P_1$ is an $n\times k$ matrix and $P_2$ is an $n \times (n-k)$ matrix.
a) What are the dimensions of $P^T$? of $P_1^T$? of $P_2^T$? Show that
$$
P^T = \left[
\begin{array}{c}
P^T_1\\
\hline
P^T_2
\end{array}
\right]
$$
b) Let $\vec{y}$ be an $n\times 1$ vector. Show that
$$
P^T\vec{y} = \left[
\begin{array}{c}
P^T_1\vec{y}\\
\hline
P^T_2\vec{y}
\end{array}
\right]
$$
What are the dimensions of $P^T_1\vec{y}$ and $P^T_2\vec{y}$?
c) Let $A$ be an $n \times n$ matrix. Argue that
$$
P^TAP = \left[
\begin{array}{c|c}
P^T_1AP_1 & P^T_1AP_2\\
\hline
P^T_2AP_1 & P^T_2AP_2
\end{array}
\right]
$$
What are the dimensions of each of these submatrices?

a) The dimensions of $P^T$, $P_1^T$, and $P_2^T$ are $n \times n$, $k\times n$, and $(n-k)\times n$ respectively.

We can show that that $P^T$ is composed of $P_1^T$ stacked on top of $P_2^T$. We know that $P_1$ is composed of the $k$ columns of $P$ and all the rows, so when we transpose, we get a matrix that now has $k$ rows and $n$ columns. This represents the first $k$ rows of $P^T$. $P_2^T$ represents the bottom $n-k$ rows of $P^T$ so if we "tack it on underneath" $P_1^T$ then the two partitions constitute the entirety of $P^T$.

b) Since $P^T$ is $n\times n$ and $\vec{y}$ is $n\times 1$ we know that the resulting matrix multiplication will result in a new $n\times 1$ matrix (column vector). We also know that $P_1^T$ is $k\times n$ so multiplying it with $\vec{y}$ will result in a $k\times 1$ vector. The product of $P_2^T$ and $\vec{y}$ will result in a $(n-k)\times 1$ vector. When we put the two vectors "back together" stacking $P_1^T\vec{y}$ on $P_2^T\vec{y}$ we get the same $n\times 1$ vector we would get is we just multiplied $P^T$ by $\vec{y}$.

c) We can make a very similar argument for $P^TAP$. The dimensions of the partitions are:

$P^T_1AP_1$ is $k\times k$
$P^T_1AP_2$ is $k\times (n-k)$
$P^T_2AP_1$ is $(n-k)\times k$
$P^T_2AP_2$ is $(n-k)\times (n-k)$

Combining these individual matrices together produces the expected $n\times n$ matrix.